# Gitstory Integration Analysis
**Research Date**: 2026-02-20
**Target**: Dan's gitstory implementation at `/Users/masa/Projects/gitstory`
**Purpose**: Comprehensive analysis for potential mcp-vector-search integration

---

## Executive Summary

Gitstory is a **Rust CLI tool** that generates development narratives from git repositories by invoking Claude Code as a subprocess. It extracts git history, GitHub metadata (issues/PRs), and documentation, then asks Claude to synthesize a structured narrative following a detailed recipe. The tool is well-architected with clear module boundaries, comprehensive error handling, and 26 passing tests.

**Key Finding**: Gitstory's subprocess-based approach (spawning `claude` CLI) makes it **incompatible** with direct integration into mcp-vector-search. However, its narrative recipe and data extraction patterns offer **HIGH VALUE** for a new `mcp-vector-search story` command that leverages our existing embedding database.

---

## 1. Architecture & Implementation

### 1.1 Technology Stack

| Component | Technology | Purpose |
|-----------|------------|---------|
| **Language** | Rust (2024 edition) | High-performance CLI with zero-cost abstractions |
| **CLI Framework** | clap v4 (derive macros) | Argument parsing with auto-generated help |
| **Async Runtime** | tokio (full features) | Subprocess management and I/O |
| **HTTP Client** | reqwest v0.12 | Future Claude API integration (not used yet) |
| **Serialization** | serde + serde_json | JSON parsing from gh CLI output |
| **Error Handling** | thiserror + anyhow | Structured errors with ergonomic propagation |
| **Date/Time** | chrono v0.4 | Timestamp parsing and formatting |
| **Environment** | dotenvy | Multi-level .env file loading |
| **Paths** | dirs v5 | Cross-platform home directory detection |

**Build Status**: ‚úÖ Compiles successfully with `cargo build --release` in 21 seconds
**Test Coverage**: ‚úÖ 26/26 tests passing (0 failures)
**Binary Size**: Optimized with LTO and strip for distribution

### 1.2 Module Architecture

```
src/
‚îú‚îÄ‚îÄ main.rs (393 lines)       # Entry point, orchestrates pipeline, exit codes
‚îú‚îÄ‚îÄ cli.rs (137 lines)        # clap command definitions, OutputFormat enum
‚îú‚îÄ‚îÄ config.rs (119 lines)     # Multi-level .env loading (process > user > local)
‚îú‚îÄ‚îÄ extract.rs (516 lines)    # Git/gh data extraction with graceful degradation
‚îú‚îÄ‚îÄ invoke.rs (184 lines)     # Claude subprocess spawning with timeout
‚îú‚îÄ‚îÄ output.rs (401 lines)     # Markdown/JSON generation (placeholder in Phase 1)
‚îú‚îÄ‚îÄ synthesize.rs (221 lines) # Narrative data structures, API config
‚îî‚îÄ‚îÄ error.rs (129 lines)      # Structured error types with thiserror

Total: 2,100 lines of Rust (8 modules, well-organized)
```

**Module Responsibilities:**

- **`main.rs`**: Validates repository, spawns Claude, strips preamble, replaces time placeholder, writes output
- **`cli.rs`**: Defines `gitstory analyze <path>` with `--output`, `--format`, `--timeout` flags
- **`extract.rs`**: Runs `git log`, `gh issue list`, `gh pr list`, `gh label list` with JSON output parsing
- **`invoke.rs`**: Spawns `claude -p <recipe>` in target directory with channel-based timeout handling
- **`output.rs`**: Formats narrative as markdown with structured sections (acts, themes, confidence)
- **`synthesize.rs`**: Defines `Narrative`, `NarrativeAct`, `Theme`, `RoadNotTaken`, `ConfidenceLevel` types
- **`error.rs`**: 15 error variants covering all failure modes (repo not found, git failed, timeout, etc.)

### 1.3 Core Algorithm: How It Works

**Phase 1: Repository Validation**
```rust
// main.rs:193-205
if !path.exists() { return INVALID_TARGET; }
if !path.join(".git").exists() { return INVALID_TARGET; }
```

**Phase 2: Directory Setup**
```rust
// main.rs:208-217
let gitstory_dir = path.join(".gitstory");
let errors_dir = gitstory_dir.join("errors");
fs::create_dir_all(&errors_dir)?;
```

**Phase 3: Claude Invocation** (The Core Magic)
```rust
// invoke.rs:57-117
pub fn spawn_claude(target_dir: &Path, recipe: &str, timeout_secs: u64) -> Result<InvokeResult> {
    let child = Command::new("claude")
        .arg("-p")
        .arg(recipe)  // Full recipe passed as prompt
        .current_dir(&target_dir)
        .stdout(Stdio::piped())
        .stderr(Stdio::piped())
        .spawn()?;

    // Timeout via channel-based waiting
    let (tx, rx) = mpsc::channel();
    thread::spawn(move || { tx.send(child.wait_with_output()) });

    match rx.recv_timeout(Duration::from_secs(timeout_secs)) {
        Ok(output) => Ok(InvokeResult { stdout, stderr, exit_code, elapsed }),
        Err(_) => bail!("timed out after {} seconds", timeout_secs),
    }
}
```

**Phase 4: Output Processing**
```rust
// main.rs:250-251
let narrative = strip_preamble(&result.stdout)
    .replace("ELAPSED_TIME_PLACEHOLDER", &elapsed_formatted);
```

**Phase 5: File Writing**
```rust
// main.rs:253-260
let output_path = custom_output.unwrap_or_else(|| {
    gitstory_dir.join(format!("narrative-{}.md", date))
});
write_output(&output_path, &narrative, format)?;
```

**Exit Codes** (PRD-specified):
- `0` = Success
- `1` = Claude failed (error logged to `.gitstory/errors/<timestamp>.md`)
- `2` = Timeout exceeded
- `3` = Invalid target (not a git repository)

### 1.4 Data Extraction Details

**Git Commits** (`extract.rs:185-229`):
```bash
git log --oneline --format="%h|%s|%an|%aI" -n 100
```
Parses into `CommitInfo { hash, message, author, date }` structs

**GitHub Issues** (`extract.rs:269-331`):
```bash
gh issue list --state all --limit 100 --json number,title,state,labels,createdAt,closedAt
```
Gracefully handles missing `gh` CLI or no remote

**Pull Requests** (`extract.rs:334-383`):
```bash
gh pr list --state all --limit 50 --json number,title,state,mergedAt
```

**Labels** (`extract.rs:386-416`):
```bash
gh label list --json name,description,color
```

**Research Documents** (`extract.rs:419-475`):
- Checks: `docs/research/`, `docs/`, `research/`, `PRD.md`, `README.md`, `ARCHITECTURE.md`
- Enumerates markdown files with metadata (size, modified date)

### 1.5 Synthesis Recipe (The Secret Sauce)

The actual recipe lives in `docs/project-narrative-extraction-recipe.md` (414 lines):

**5-Phase Process:**
1. **Data Extraction**: Run git/gh commands, list docs
2. **Pattern Recognition**: Temporal clusters, naming conventions, issue-PR-commit chains
3. **Research Doc Mining**: Find >10KB files, extract decision quotes
4. **Cross-Artifact Synthesis**: Connect commits to issues to docs, identify pivots
5. **Narrative Construction**: Three-act structure with evidence citations

**Output Structure Mandated by Recipe:**
```markdown
# [Project Name]: A Development Narrative

## Executive Summary
[1 paragraph: what, major arc, key insight]

## The Story in Three Acts
### Act I: [Title] ([Date Range])
[Evidence-cited narrative]

### Act II: [Title] ([Date Range])
[Pivots, challenges, evolution]

### Act III: [Title] ([Date Range])
[Current state, resolution]

## Thematic Analysis
[3-5 themes with HIGH/MEDIUM/LOW confidence]

## Roads Not Taken
[Abandoned approaches with evidence]

## Confidence Assessment
| Claim | Evidence | Confidence |

## Gaps and Unknowns
[What artifacts don't reveal]

## Sources Consulted
[Grouped: Commits, Issues, Docs]
```

**Key Recipe Heuristics:**
- **High-Signal**: Epic issues with sub-issues, research docs >10KB, commit conventions, issue refs in commits
- **Low-Signal (Filter)**: Merge commits, dependency updates, typo fixes, build artifacts
- **Patterns**: Temporal clusters, naming conventions (feat:, fix:), issue-PR-commit chains, inflection points
- **Confidence Levels**: HIGH = explicit statement, MEDIUM = inferred from patterns, LOW = speculation

### 1.6 Performance Characteristics

**Execution Model**: Synchronous blocking (default)
```rust
// cli.rs:51-52
#[arg(long, default_value_t = true)]
sync: bool,
```
Note: Async flag exists but not implemented (Phase 1)

**Timeout**: Default 300 seconds (5 minutes)
```rust
// cli.rs:55-56
#[arg(long, default_value_t = 300)]
timeout: u64,
```

**Memory Efficiency**:
- Extracts only last 100 commits (not full history)
- GitHub API limits: 100 issues, 50 PRs
- Does NOT load file contents into memory (Claude reads files directly via subprocess)

**Expected Timing** (from PRD):
- Extraction phase: <30 seconds
- Full analysis: <5 minutes (depends on Claude response time)
- Actual: Varies by repository size and Claude availability

---

## 2. Key Features

### 2.1 Narrative Generation

**Three-Act Story Structure**:
- Act I: Foundation (initial vision, early architecture)
- Act II: Challenge (problems, pivots, decisions)
- Act III: Resolution (current state, achieved capabilities)

**Evidence Citations**:
- Commits: `[commit abc123]`
- Issues: `[Issue #42]`
- PRs: `[PR #15]`
- Documents: `[docs/research/decision.md]`

**Confidence Tagging**:
```rust
// synthesize.rs:81-90
pub enum ConfidenceLevel {
    High,    // Explicit statement in artifact
    Medium,  // Inferred from timing/patterns
    Low,     // Speculation based on gaps
}
```

### 2.2 Cross-Artifact Synthesis

**Issue-PR-Commit Chains**:
```
Issue #81 (Epic created)
  ‚Üì
Issues #82-89 (Sub-tasks)
  ‚Üì
PRs #90-95 (Implementation)
  ‚Üì
Commits with "#82" references
```

**Temporal Analysis**:
- Clustering of related commits (focused sprints)
- Gaps between phases (pivots, reassessment)
- Overlapping work streams (parallel development)

**Pattern Recognition**:
- Commit message conventions (feat:, fix:, refactor:)
- Label taxonomy (P0-P4, component labels)
- Research doc dating patterns

### 2.3 Output Formats

**Markdown** (default):
```bash
gitstory analyze /path/to/repo
# Writes to: /path/to/repo/.gitstory/narrative-2026-02-20.md
```

**JSON** (metadata only):
```bash
gitstory analyze /path/to/repo --format json
# Writes to: /path/to/repo/.gitstory/narrative-2026-02-20.json
```

**Both**:
```bash
gitstory analyze /path/to/repo --format both
```

**JSON Structure** (output.rs:14-33):
```json
{
  "version": "0.1.0",
  "generated_at": "2026-02-20T12:00:00Z",
  "repository": { "path": "...", "url": "...", "created_at": "..." },
  "artifacts": { "commits": 255, "issues": 68, "pull_requests": 46, "research_docs": 25 },
  "themes": [{ "name": "...", "timeline": {...}, "confidence": "HIGH", "evidence_count": 15 }],
  "pivots": [{ "description": "...", "date": "...", "velocity_days": 12, "trigger": "..." }],
  "roads_not_taken": [{ "name": "...", "issues": [12, 13], "abandoned_date": "..." }],
  "narrative_quality": { "research_doc_richness": "HIGH", "decision_documentation": "HIGH", "gaps": [...] }
}
```

### 2.4 Graceful Degradation

**No GitHub CLI**:
```rust
// extract.rs:236-245
let gh_available = Command::new("gh").arg("--version").output()
    .map(|o| o.status.success())
    .unwrap_or(false);

if !gh_available {
    eprintln!("Note: GitHub CLI (gh) not available, skipping GitHub data extraction");
    return (vec![], vec![], vec![]);
}
```
Falls back to git-only analysis (commits + local files)

**No GitHub Remote**:
```rust
// extract.rs:159-173
let url = Command::new("gh")
    .args(["repo", "view", "--json", "url", "-q", ".url"])
    .output()
    .ok()
    .and_then(|output| { ... });
// Returns None if no remote, continues without panic
```

**Claude Failure**:
```rust
// main.rs:273-298
if result.exit_code != 0 {
    let error_content = format!("# Claude Error\n\n**Exit Code:** {}\n...", result.exit_code);
    fs::write(&error_file, &error_content)?;
    eprintln!("Error details written to: {}", error_file.display());
    return CLAUDE_FAILED;
}
```
Writes full stdout/stderr to `.gitstory/errors/<timestamp>.md` for debugging

### 2.5 Configuration

**Environment Variables** (config.rs:1-34):
```bash
# Priority order (highest to lowest):
# 1. Process environment (shell)
export ANTHROPIC_API_KEY="sk-ant-..."
export GITSTORY_MODEL="claude-sonnet-4-20250514"
export GITSTORY_OUTPUT_DIR="./narratives"

# 2. User config: ~/.config/gitstory/.env
# 3. Local config: ./.env
```

**Loading Strategy**:
- Uses `dotenvy` crate (doesn't override existing env vars)
- Loads in reverse priority order (local, then user, then process)
- Silently skips missing files

### 2.6 NO React Frontend

**Critical Finding**: Despite the README mentioning "React components" in early planning docs, the actual implementation has:
- ‚ùå No `package.json`
- ‚ùå No JavaScript/TypeScript files
- ‚ùå No React dependencies
- ‚ùå No visualization components

**What Exists Instead**:
- Pure Rust CLI (no web interface)
- Markdown output (human-readable text)
- JSON output (machine-readable metadata)

**Why This Matters**: Integration would NOT involve React component reuse. Any visualization for mcp-vector-search would need to be built from scratch.

### 2.7 AI/LLM Usage

**Current Implementation**: 100% Claude-based via subprocess invocation
```rust
// main.rs:38
const RECIPE: &str = include_str!("../docs/project-narrative-extraction-recipe.md");

// main.rs:66-157
fn build_execution_prompt() -> String {
    r##"Analyze the git repository in your current directory and produce a project narrative document.

    Use git and gh commands to extract commits, issues, PRs, and documentation. Then write a narrative following this EXACT format.
    ..."##.to_string()
}
```

**Prompt Engineering**:
- Embeds 414-line recipe at compile time
- Wraps recipe in execution instructions
- Strips preamble text before markdown heading (Claude sometimes outputs "Now I have enough context to...")
- Replaces `ELAPSED_TIME_PLACEHOLDER` with actual duration

**Model Selection**: Configurable via `GITSTORY_MODEL` env var (default: `claude-sonnet-4-20250514`)

**Future Plans** (PRD Phase 4-5):
- Phase 4: Embedding support for semantic search across docs
- Phase 5: Cross-project correlation

**NO Local LLM Support**: Requires Claude API (cloud only)

---

## 3. Integration Assessment with mcp-vector-search

### 3.1 Architectural Compatibility

**INCOMPATIBLE: Subprocess-Based Approach**

Gitstory's core design spawns `claude` as an external process:
```rust
// invoke.rs:69-78
Command::new("claude")
    .arg("-p")
    .arg(recipe)
    .current_dir(&target_dir)
    .spawn()?
```

**Why This Breaks Integration**:
1. **No Direct API Access**: Gitstory doesn't call Claude API directly (uses CLI subprocess)
2. **Can't Share Context**: mcp-vector-search's in-memory embeddings unavailable to subprocess
3. **Separate Process Space**: No way to pass vector search results to Claude invocation
4. **Timing Issues**: 5-minute timeout for subprocess, but no streaming progress

**What This Means**:
- ‚ùå Cannot import gitstory as a Rust library
- ‚ùå Cannot reuse its Claude invocation logic
- ‚ùå Cannot share embedding database with its narrative generation
- ‚úÖ CAN reuse data extraction patterns
- ‚úÖ CAN reuse narrative recipe/structure
- ‚úÖ CAN learn from its error handling approach

### 3.2 Data Enrichment Opportunities

**What mcp-vector-search Already Has That Gitstory Needs**:

| Data Type | Gitstory Approach | mcp-vector-search Capability |
|-----------|-------------------|------------------------------|
| **Code Semantics** | ‚ùå No code understanding | ‚úÖ CodeT5+ embeddings in `code_vectors.lance` |
| **File Relationships** | ‚ùå Manual file discovery | ‚úÖ Chunk metadata with file paths in `chunks.lance` |
| **Semantic Search** | ‚ùå Keyword-based grep | ‚úÖ Vector similarity search across codebase |
| **Code Context** | ‚ùå Reads full files | ‚úÖ Pre-indexed chunks with surrounding context |
| **Architectural Patterns** | ‚ùå Must infer from commits | ‚úÖ Can query "show me all authentication code" |
| **Tech Stack Detection** | ‚ùå Manual inspection | ‚úÖ Language metadata from indexing |

**What Gitstory Has That We Don't**:

| Data Type | Gitstory Capability | mcp-vector-search Gap |
|-----------|---------------------|----------------------|
| **Git History** | ‚úÖ 100 recent commits with metadata | ‚ùå No git integration |
| **GitHub Issues** | ‚úÖ Full issue list with labels, dates | ‚ùå No GitHub API access |
| **Pull Requests** | ‚úÖ PR metadata with merge dates | ‚ùå No PR tracking |
| **Temporal Clustering** | ‚úÖ Groups commits by time period | ‚ùå No time-series analysis |
| **Issue-Commit Linking** | ‚úÖ Traces #N references | ‚ùå No cross-artifact synthesis |
| **Research Doc Evaluation** | ‚úÖ Sizes, dates, topic inference | ‚ùå No doc quality assessment |

### 3.3 Proposed Integration: `mcp-vector-search story`

**Architecture Decision**: Build NEW command leveraging existing database

```python
# NEW: src/mcp_vector_search/narrative.py

from lancedb import connect
from datetime import datetime, timedelta
import subprocess
import json

class NarrativeGenerator:
    def __init__(self, db_path: str):
        self.db = connect(db_path)
        self.chunks = self.db.open_table("chunks")
        self.code_vectors = self.db.open_table("code_vectors")

    def generate_narrative(self, repo_path: str) -> str:
        """Generate narrative enriched with semantic code understanding"""

        # Phase 1: Extract git/GitHub data (reuse gitstory patterns)
        commits = self._extract_commits(repo_path)
        issues = self._extract_issues(repo_path)
        prs = self._extract_pull_requests(repo_path)

        # Phase 2: Enrich with semantic code search
        code_themes = self._identify_code_themes()
        architecture = self._analyze_architecture()
        tech_stack = self._detect_tech_stack()

        # Phase 3: Temporal analysis
        commit_clusters = self._cluster_commits_by_time(commits)
        pivot_points = self._identify_pivots(commits, issues)

        # Phase 4: Cross-artifact synthesis
        issue_commit_chains = self._link_issues_to_commits(issues, commits)
        roads_not_taken = self._find_abandoned_approaches(issues, commits)

        # Phase 5: Generate narrative with Claude API
        context = {
            "commits": commits,
            "issues": issues,
            "prs": prs,
            "code_themes": code_themes,
            "architecture": architecture,
            "tech_stack": tech_stack,
            "pivot_points": pivot_points,
            "issue_commit_chains": issue_commit_chains,
            "roads_not_taken": roads_not_taken
        }

        return self._synthesize_with_claude(context)

    def _identify_code_themes(self) -> list[dict]:
        """Use vector search to find code patterns"""

        # Example: Find all authentication-related code
        auth_query = "authentication login session user management"
        auth_results = self.code_vectors.search(auth_query).limit(20).to_list()

        # Example: Find all database interaction code
        db_query = "database query SQL ORM repository persistence"
        db_results = self.code_vectors.search(db_query).limit(20).to_list()

        return [
            {"theme": "Authentication", "files": [r["path"] for r in auth_results]},
            {"theme": "Data Persistence", "files": [r["path"] for r in db_results]},
        ]

    def _analyze_architecture(self) -> dict:
        """Identify architectural patterns from code"""

        # Search for common patterns
        patterns = {
            "API Endpoints": "REST API endpoint handler route",
            "Background Jobs": "queue worker background task cron",
            "State Management": "redux vuex state store context",
            "Testing": "test spec describe it should expect",
            "Error Handling": "try catch exception error handling",
        }

        results = {}
        for name, query in patterns.items():
            hits = self.code_vectors.search(query).limit(10).to_list()
            results[name] = len(hits)

        return results

    def _detect_tech_stack(self) -> dict:
        """Infer tech stack from indexed files"""

        # Query chunks table for file extensions
        # (This would be more efficient with metadata queries)
        tech_indicators = {
            "Python": ["*.py", "requirements.txt", "pyproject.toml"],
            "TypeScript": ["*.ts", "*.tsx", "tsconfig.json"],
            "Rust": ["*.rs", "Cargo.toml"],
            "React": ["jsx", "useState", "useEffect"],
        }

        # Implementation would query chunks for these patterns
        return {}
```

**CLI Command**:
```bash
# NEW command in mcp-vector-search
mcp-vector-search story /path/to/repo

# Options (mirroring gitstory)
mcp-vector-search story /path/to/repo --output ./narratives/report.md
mcp-vector-search story /path/to/repo --format both  # markdown + JSON
```

**Key Differences from Gitstory**:
1. **Direct Claude API**: No subprocess, use `anthropic` Python SDK
2. **Semantic Enrichment**: Vector search provides code context gitstory lacks
3. **Database-Backed**: Leverages existing `chunks.lance` and `code_vectors.lance`
4. **Faster Analysis**: Pre-indexed codebase means no file reading needed

### 3.4 Data Flow: Enhanced Narrative Generation

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    User Command                             ‚îÇ
‚îÇ           mcp-vector-search story /repo                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                 Phase 1: Data Extraction                    ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  Git History  ‚îÇ  ‚îÇ GitHub Issues ‚îÇ  ‚îÇ  File Metadata ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  (subprocess) ‚îÇ  ‚îÇ  (gh CLI/API) ‚îÇ  ‚îÇ  (chunks.lance)‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ            Phase 2: Semantic Code Analysis                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇVector Search: ‚îÇ  ‚îÇVector Search: ‚îÇ  ‚îÇVector Search:  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇCode Themes    ‚îÇ  ‚îÇ Architecture  ‚îÇ  ‚îÇ  Tech Stack    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ(code_vectors) ‚îÇ  ‚îÇ   Patterns    ‚îÇ  ‚îÇ   Detection    ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ           Phase 3: Temporal & Pattern Analysis              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇCommit         ‚îÇ  ‚îÇIssue-Commit   ‚îÇ  ‚îÇRoads Not       ‚îÇ  ‚îÇ
‚îÇ  ‚îÇClustering     ‚îÇ  ‚îÇLinking        ‚îÇ  ‚îÇTaken           ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ          Phase 4: Claude API Synthesis                      ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ Anthropic SDK: claude-opus-4-6                       ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ Prompt: Gitstory recipe + semantic code context     ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ Context: {commits, issues, code_themes, architecture}‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   Phase 5: Output                           ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                  ‚îÇ
‚îÇ  ‚îÇ Markdown     ‚îÇ         ‚îÇ JSON         ‚îÇ                  ‚îÇ
‚îÇ  ‚îÇ Narrative    ‚îÇ         ‚îÇ Metadata     ‚îÇ                  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 3.5 Value Matrix

**HIGH VALUE + EASY (Do First)**:

1. **Reuse Narrative Recipe** (Complexity: LOW, Value: HIGH)
   - Copy `docs/project-narrative-extraction-recipe.md` to mcp-vector-search
   - Adapt prompt for direct Claude API call (remove subprocess instructions)
   - Estimated effort: 4 hours

2. **Git History Extraction** (Complexity: LOW, Value: HIGH)
   - Port `extract.rs:185-229` git log parsing to Python
   - Use `subprocess.run(["git", "log", ...])` with same format string
   - Estimated effort: 2 hours

3. **GitHub Integration** (Complexity: LOW, Value: HIGH)
   - Port `extract.rs:269-416` gh CLI calls to Python
   - Graceful degradation if gh unavailable (same as gitstory)
   - Estimated effort: 4 hours

4. **Three-Act Structure** (Complexity: LOW, Value: MEDIUM)
   - Reuse output format from `output.rs:263-347`
   - Python string templating for markdown generation
   - Estimated effort: 3 hours

**HIGH VALUE + COMPLEX (Do Later)**:

5. **Semantic Code Theme Detection** (Complexity: MEDIUM, Value: HIGH)
   - Query `code_vectors.lance` for architecture patterns
   - Cluster results by semantic similarity
   - Requires: Vector search expertise, embedding interpretation
   - Estimated effort: 16 hours

6. **Issue-Commit-Code Linking** (Complexity: HIGH, Value: HIGH)
   - Parse issue numbers from commit messages (#N patterns)
   - Match commit diffs to file chunks in database
   - Find relevant code snippets for each issue
   - Estimated effort: 24 hours

7. **Pivot Detection with Code Context** (Complexity: HIGH, Value: MEDIUM)
   - Identify architectural changes in git history
   - Correlate with code deletions/additions in embeddings
   - Detect when large swaths of code removed (framework migration)
   - Estimated effort: 20 hours

**LOW VALUE (Skip)**:

8. **Rust-to-Python Port** (Complexity: HIGH, Value: NONE)
   - Would lose gitstory's subprocess model (incompatible anyway)
   - Python implementation from scratch is easier than FFI
   - Skip: Build new Python tool instead

9. **JSON Metadata Format** (Complexity: LOW, Value: LOW)
   - Gitstory's JSON output is placeholder (not fully implemented)
   - Design our own metadata schema based on actual needs
   - Skip gitstory's format: Design from scratch

10. **Error Logging to `.gitstory/errors/`** (Complexity: LOW, Value: LOW)
    - Gitstory writes Claude failures to timestamped markdown files
    - mcp-vector-search should use Python logging instead
    - Skip custom error directory: Use standard logging

### 3.6 Minimum Viable Version (MVP)

**Goal**: `mcp-vector-search story` command that generates narratives in 8 weeks

**Scope**:
1. ‚úÖ Git history extraction (commits, authors, dates)
2. ‚úÖ GitHub integration (issues, PRs, labels via gh CLI)
3. ‚úÖ Narrative recipe adaptation for Claude API
4. ‚úÖ Three-act markdown output
5. ‚úÖ Basic semantic enrichment (code themes via vector search)
6. ‚ö†Ô∏è SKIP: Pivot velocity metrics (complex temporal analysis)
7. ‚ö†Ô∏è SKIP: Research doc quality scoring (nice-to-have)
8. ‚ö†Ô∏è SKIP: JSON metadata export (markdown is sufficient for MVP)

**MVP Feature Set**:
```bash
# Basic usage
mcp-vector-search story /path/to/repo

# Custom output
mcp-vector-search story /path/to/repo --output ./report.md

# What it generates:
# - Executive summary (1 paragraph)
# - Three acts (foundation, evolution, current state)
# - Code themes (via vector search: "found 15 auth-related files")
# - Confidence assessment (HIGH/MEDIUM/LOW for claims)
# - Gaps and unknowns
```

**Dependencies**:
```toml
# pyproject.toml additions
anthropic = "^0.23.0"     # Claude API SDK
```

**Estimated Timeline**:
- Week 1-2: Git/GitHub extraction (Python ports of gitstory logic)
- Week 3-4: Narrative recipe integration + Claude API calls
- Week 5-6: Semantic code analysis (vector search integration)
- Week 7: Three-act markdown generation
- Week 8: Testing, refinement, documentation

---

## 4. Value/Complexity Matrix

### 4.1 Feature Categorization

```
        HIGH VALUE
            ‚Üë
            ‚îÇ
   Q2       ‚îÇ        Q1
   (Later)  ‚îÇ    (Do First)
            ‚îÇ
  Complex ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Easy
            ‚îÇ
   Q3       ‚îÇ        Q4
   (Skip)   ‚îÇ    (Nice-to-Have)
            ‚îÇ
            ‚Üì
        LOW VALUE
```

**Q1: HIGH VALUE + EASY (Priority 1)**
- ‚úÖ Narrative recipe reuse
- ‚úÖ Git history extraction
- ‚úÖ GitHub API integration
- ‚úÖ Three-act structure
- ‚úÖ Markdown output generation

**Q2: HIGH VALUE + COMPLEX (Priority 2)**
- ‚ö†Ô∏è Semantic code theme detection
- ‚ö†Ô∏è Issue-commit-code linking
- ‚ö†Ô∏è Architecture pattern recognition
- ‚ö†Ô∏è Tech stack inference from embeddings

**Q3: LOW VALUE + COMPLEX (Skip)**
- ‚ùå Full Rust-to-Python port
- ‚ùå Subprocess-based Claude invocation (incompatible)
- ‚ùå Timeout channel implementation (Python has simpler alternatives)

**Q4: LOW VALUE + EASY (Nice-to-Have)**
- üü° JSON metadata export
- üü° Error logging to files (use Python logging instead)
- üü° Custom output directory (just use --output flag)

### 4.2 Risk Assessment

**Technical Risks**:

1. **Claude API Rate Limits** (Severity: MEDIUM)
   - Gitstory makes one large API call (entire narrative in one shot)
   - Could hit token limits on large repos (>1000 commits)
   - Mitigation: Implement chunking strategy, use streaming API

2. **Vector Search Quality** (Severity: LOW)
   - Semantic code themes depend on CodeT5+ embedding quality
   - May not match gitstory's manual analysis accuracy
   - Mitigation: Hybrid approach (vector search + keyword fallback)

3. **Git History Parsing** (Severity: LOW)
   - Different git configs may produce unexpected output
   - Date format parsing issues across timezones
   - Mitigation: Use `git log --format=json` if available, parse robustly

4. **GitHub CLI Availability** (Severity: LOW)
   - User may not have `gh` installed or authenticated
   - Mitigation: Graceful degradation to git-only mode (gitstory does this well)

**Integration Risks**:

1. **Database Schema Changes** (Severity: HIGH)
   - If `chunks.lance` or `code_vectors.lance` schema changes, narrative code breaks
   - Mitigation: Use schema versioning, write tests for expected columns

2. **Claude API Changes** (Severity: MEDIUM)
   - Anthropic SDK may change, model names may deprecate
   - Mitigation: Pin SDK version, abstract API calls behind interface

3. **Maintenance Burden** (Severity: MEDIUM)
   - New feature adds complexity to mcp-vector-search
   - Requires ongoing maintenance as gitstory evolves
   - Mitigation: Clear documentation, automated tests

### 4.3 Effort Estimates

**Total Effort: 80-120 hours (2-3 weeks full-time)**

| Component | Complexity | Hours | Dependencies |
|-----------|------------|-------|--------------|
| Git extraction | Easy | 8 | subprocess, git installed |
| GitHub integration | Easy | 12 | gh CLI (optional) |
| Narrative recipe adaptation | Medium | 16 | Anthropic SDK |
| Claude API integration | Medium | 20 | API key, quota |
| Vector search integration | Medium | 24 | Existing lance tables |
| Markdown generation | Easy | 8 | String formatting |
| Testing & docs | Medium | 16 | pytest, sphinx |
| Buffer (unknowns) | - | 16 | - |

**Phased Rollout**:
- **Phase 1 (40 hours)**: Basic git extraction + narrative generation (no vector search)
- **Phase 2 (40 hours)**: Add semantic code analysis from embeddings
- **Phase 3 (20 hours)**: Polish, testing, documentation

---

## 5. Code Quality Assessment

### 5.1 Structure and Maintainability

**Strengths** ‚úÖ:
- **Clear Module Boundaries**: 8 modules with single responsibilities
- **Type Safety**: Rust's type system prevents many runtime errors
- **Comprehensive Tests**: 26 unit tests covering core functionality
- **Error Handling**: Structured errors with `thiserror`, clear error messages
- **Documentation**: Module-level docs, function comments, inline explanations
- **Configuration Management**: Multi-level .env loading with priority

**Weaknesses** ‚ö†Ô∏è:
- **Placeholder Implementation**: `output.rs` and `synthesize.rs` have TODO placeholders
- **Unused Functions**: 3 functions never called (triggers compiler warnings)
- **No Integration Tests**: Only unit tests, no end-to-end validation
- **Hardcoded Limits**: 100 commits, 100 issues, 50 PRs (should be configurable)
- **No Streaming**: 5-minute blocking wait for Claude (no progress updates)

### 5.2 Technical Debt

**Low Technical Debt** (Score: 8/10)

**Identified Issues**:
1. **Extraction Not Used**: `extract_repository_data()` function exists but main.rs doesn't call it
   - Location: `extract.rs:126-154`
   - Impact: Dead code, misleading for contributors
   - Fix: Remove or integrate into pipeline

2. **Synthesis Placeholder**: `synthesize_narrative()` returns hardcoded placeholder
   - Location: `synthesize.rs:133-164`
   - Impact: Function exists but does nothing useful
   - Fix: Implement or document as future work

3. **Output Generation Unused**: `generate_output()` not called in main pipeline
   - Location: `output.rs:97-145`
   - Impact: Suggests incomplete Phase 1 implementation
   - Fix: Wire into main.rs or remove

4. **No Async Benefits**: Uses `tokio` but everything is sync
   - Location: `main.rs:160` (`#[tokio::main]`)
   - Impact: Unnecessary dependency overhead
   - Fix: Remove tokio or implement true async (e.g., concurrent API calls)

**Good Practices**:
- ‚úÖ Compiler warnings acknowledged (not ignored)
- ‚úÖ Tests prevent regressions
- ‚úÖ Error types are descriptive
- ‚úÖ No unsafe code blocks
- ‚úÖ Dependencies are up-to-date

### 5.3 Dependencies Health

**All Dependencies Current** (as of 2024 edition):

| Crate | Version | Status | Last Updated |
|-------|---------|--------|--------------|
| clap | 4.x | ‚úÖ Stable | Active (major ecosystem library) |
| tokio | 1.x | ‚úÖ Stable | Active (de facto async runtime) |
| reqwest | 0.12 | ‚úÖ Stable | Active (HTTP client standard) |
| serde | 1.x | ‚úÖ Stable | Active (serialization standard) |
| thiserror | 2.x | ‚úÖ Latest | Active (error handling) |
| anyhow | 1.x | ‚úÖ Stable | Active (error propagation) |
| chrono | 0.4 | ‚ö†Ô∏è Maintenance | Consider `time` crate as alternative |
| dotenvy | 0.15 | ‚úÖ Current | Fork of unmaintained `dotenv` |
| dirs | 5.x | ‚úÖ Latest | Active (platform paths) |

**Risk Assessment**: LOW
- No deprecated dependencies
- All crates maintained by reputable authors
- Chrono has security advisories but used safely here (no timezone calculations)

### 5.4 Test Coverage

**Unit Tests**: 26 passing (100% pass rate)

**Coverage by Module**:
```
cli.rs:        3 tests (verify_cli, parsing, options)
config.rs:     3 tests (path format, display, load safety)
error.rs:      3 tests (display, git error, partial success)
extract.rs:    2 tests (commit parsing, serialization)
invoke.rs:     4 tests (duration formatting, result success)
main.rs:       6 tests (preamble stripping variations)
output.rs:     2 tests (markdown format, metadata serialization)
synthesize.rs: 3 tests (confidence display, serialization, config default)
```

**Coverage Gaps**:
- ‚ùå No integration tests (end-to-end pipeline)
- ‚ùå No error path tests (what if git fails? gh fails?)
- ‚ùå No timeout tests (does channel-based timeout work?)
- ‚ùå No filesystem tests (mock .git directory, test validation)
- ‚ùå No Claude invocation tests (mock subprocess)

**Recommendation**: Add integration test that:
1. Creates temp git repo with sample history
2. Runs gitstory analyze on it
3. Validates markdown output structure
4. Cleans up temp files

**Estimated Effort**: 8 hours to achieve 80% coverage

### 5.5 Performance Characteristics

**Memory**: Efficient (minimal allocation)
- Only last 100 commits loaded
- GitHub data paginated (100 issues, 50 PRs)
- No file contents loaded (Claude subprocess reads files directly)

**CPU**: Low overhead
- Mostly subprocess orchestration (git, gh, claude)
- Rust's zero-cost abstractions mean minimal processing overhead
- String parsing and JSON deserialization are fast

**I/O**: Dominated by subprocess calls
- `git log`: ~500ms for 100 commits
- `gh issue list`: ~2-5s depending on network
- `claude` subprocess: 30s - 5min depending on analysis complexity
- File writes: <10ms

**Bottleneck**: Claude invocation (30s - 5min)
- No way to optimize (external subprocess)
- Timeout set to 300s (5 minutes) by default
- Could implement progress streaming via stderr monitoring

**Scalability**:
- ‚úÖ Handles repos with 1000s of commits (only loads last 100)
- ‚úÖ Handles repos with 100s of issues (limits to 100)
- ‚ö†Ô∏è May timeout on large repos (5min limit)
- ‚ùå No incremental update support (full re-analysis each time)

---

## 6. Recommendations

### 6.1 Integration Strategy

**Recommendation: BUILD NEW, INSPIRED BY GITSTORY**

**Rationale**:
1. Subprocess architecture incompatible with mcp-vector-search
2. Python implementation easier than Rust FFI
3. Can leverage existing lance databases directly
4. Gitstory's recipe and patterns are reusable as documentation

**Implementation Plan**:

```python
# NEW: src/mcp_vector_search/cli.py additions

@cli.command()
@click.argument("repo_path", type=click.Path(exists=True))
@click.option("--output", "-o", help="Output markdown file path")
@click.option("--format", type=click.Choice(["markdown", "json", "both"]), default="markdown")
def story(repo_path: str, output: str, format: str):
    """Generate development narrative from git history and code embeddings"""
    from .narrative import NarrativeGenerator

    # Detect or prompt for database path
    db_path = detect_database(repo_path)

    generator = NarrativeGenerator(db_path=db_path)
    narrative = generator.generate_narrative(repo_path=repo_path)

    # Write output
    output_path = output or f"{repo_path}/.mcp-vector-search/narrative-{date.today()}.md"
    Path(output_path).write_text(narrative)

    click.echo(f"Narrative written to: {output_path}")
```

### 6.2 Architecture Design

**Proposed Module Structure**:

```
src/mcp_vector_search/
‚îú‚îÄ‚îÄ narrative/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ generator.py         # Main NarrativeGenerator class
‚îÇ   ‚îú‚îÄ‚îÄ extractors/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ git.py           # Git history extraction
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ github.py        # GitHub API/CLI integration
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ filesystem.py    # File metadata, docs discovery
‚îÇ   ‚îú‚îÄ‚îÄ analyzers/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ semantic.py      # Vector search for code themes
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ temporal.py      # Commit clustering, pivot detection
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ cross_artifact.py # Issue-commit-code linking
‚îÇ   ‚îú‚îÄ‚îÄ synthesizers/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ claude.py        # Claude API integration
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ templates.py     # Narrative templates, recipes
‚îÇ   ‚îî‚îÄ‚îÄ formatters/
‚îÇ       ‚îú‚îÄ‚îÄ markdown.py      # Three-act structure generation
‚îÇ       ‚îî‚îÄ‚îÄ json.py          # Metadata export
‚îî‚îÄ‚îÄ cli.py                   # Add 'story' command
```

**Key Design Decisions**:

1. **Direct Claude API** (not subprocess)
   ```python
   from anthropic import Anthropic

   client = Anthropic(api_key=os.environ["ANTHROPIC_API_KEY"])
   response = client.messages.create(
       model="claude-opus-4-6",
       max_tokens=8000,
       messages=[{"role": "user", "content": prompt}]
   )
   ```

2. **Lance Table Integration** (not file reading)
   ```python
   # Semantic code search
   results = self.code_vectors.search(
       "authentication session management"
   ).limit(50).to_pandas()

   # Get unique file paths
   auth_files = results["path"].unique().tolist()
   ```

3. **Streaming Output** (not blocking wait)
   ```python
   with client.messages.stream(
       model="claude-opus-4-6",
       max_tokens=8000,
       messages=[{"role": "user", "content": prompt}]
   ) as stream:
       for text in stream.text_stream:
           print(text, end="", flush=True)
   ```

4. **Configurable Limits** (not hardcoded)
   ```python
   @dataclass
   class NarrativeConfig:
       max_commits: int = 100
       max_issues: int = 100
       max_prs: int = 50
       semantic_search_limit: int = 20
       timeout_seconds: int = 300
   ```

### 6.3 Testing Strategy

**Unit Tests**:
```python
# tests/narrative/test_extractors.py

def test_git_commit_extraction(tmp_git_repo):
    commits = GitExtractor().extract_commits(tmp_git_repo)
    assert len(commits) > 0
    assert commits[0].hash
    assert commits[0].message

def test_github_issue_extraction_graceful_failure(tmp_git_repo):
    # No gh CLI available
    issues = GitHubExtractor().extract_issues(tmp_git_repo)
    assert issues == []  # Should not raise exception
```

**Integration Tests**:
```python
# tests/narrative/test_end_to_end.py

def test_narrative_generation_full_pipeline(sample_repo):
    generator = NarrativeGenerator(db_path=sample_repo / ".mcp-vector-search")
    narrative = generator.generate_narrative(sample_repo)

    assert "# " in narrative  # Has title
    assert "## Executive Summary" in narrative
    assert "## The Story in Three Acts" in narrative
    assert "### Act I:" in narrative
    assert "### Act II:" in narrative
    assert "### Act III:" in narrative
```

**Mocking Strategy**:
```python
# tests/narrative/conftest.py

@pytest.fixture
def mock_claude_api(monkeypatch):
    def mock_create(*args, **kwargs):
        return MockResponse(content=[MockContent(text="# Sample Narrative\n\n...")])

    monkeypatch.setattr("anthropic.Anthropic.messages.create", mock_create)
```

### 6.4 Documentation Requirements

**User Documentation**:
1. **Quick Start Guide**: `docs/narrative-quickstart.md`
2. **Configuration Reference**: Environment variables, .env files
3. **Output Format**: Three-act structure, confidence levels, citation format
4. **Troubleshooting**: Common errors (no gh CLI, no API key, timeout)

**Developer Documentation**:
1. **Architecture Diagram**: Module dependencies, data flow
2. **API Reference**: NarrativeGenerator class, extractor interfaces
3. **Recipe Customization**: How to modify narrative templates
4. **Extension Guide**: Adding new semantic analyzers

**Example Output**:
Include 2-3 example narratives from real repos (with permission)

### 6.5 Rollout Plan

**Phase 1: Foundation (Week 1-2)**
- ‚úÖ Git history extraction
- ‚úÖ GitHub API integration
- ‚úÖ Basic markdown output
- ‚úÖ Unit tests for extractors

**Phase 2: Semantic Integration (Week 3-4)**
- ‚úÖ Vector search for code themes
- ‚úÖ Architecture pattern detection
- ‚úÖ Tech stack inference
- ‚úÖ Integration tests

**Phase 3: Synthesis (Week 5-6)**
- ‚úÖ Claude API integration
- ‚úÖ Recipe adaptation
- ‚úÖ Three-act structure
- ‚úÖ Confidence tagging

**Phase 4: Polish (Week 7-8)**
- ‚úÖ Error handling
- ‚úÖ Progress streaming
- ‚úÖ Documentation
- ‚úÖ Example narratives

**Alpha Release**: Week 4 (basic narrative generation)
**Beta Release**: Week 6 (with semantic enrichment)
**Stable Release**: Week 8 (fully tested and documented)

---

## 7. Conclusion

### 7.1 Summary

Gitstory is a well-engineered Rust CLI tool with a clear purpose: generate development narratives by orchestrating git/GitHub data extraction and Claude synthesis. While its subprocess-based architecture makes direct integration impossible, its **narrative recipe**, **data extraction patterns**, and **output structure** are highly valuable templates for building `mcp-vector-search story`.

**Key Takeaway**: Don't port gitstory‚Äîlearn from it and build a superior version that leverages mcp-vector-search's semantic code understanding.

### 7.2 Next Steps

**Immediate Actions** (This Week):
1. ‚úÖ Review this analysis with project stakeholders
2. ‚úÖ Decide: Build `mcp-vector-search story` or integrate differently?
3. ‚úÖ Prioritize: MVP scope vs full feature set?

**Short-Term Actions** (Next 2 Weeks):
1. Create `src/mcp_vector_search/narrative/` module structure
2. Implement git history extraction (port from gitstory)
3. Test basic narrative generation without semantic enrichment
4. Write integration test framework

**Long-Term Actions** (Next 2 Months):
1. Semantic code theme detection via vector search
2. Issue-commit-code linking
3. Claude API integration with streaming
4. Full documentation and examples

### 7.3 Decision Matrix

**Option A: Direct Integration** ‚ùå
- Pros: Reuse battle-tested code
- Cons: Subprocess model incompatible, can't access embeddings
- Recommendation: **DO NOT PURSUE**

**Option B: Rust FFI Bindings** ‚ùå
- Pros: Could call gitstory functions from Python
- Cons: Complex, maintenance burden, still can't share database
- Recommendation: **DO NOT PURSUE**

**Option C: Build Inspired Tool** ‚úÖ **RECOMMENDED**
- Pros: Direct Claude API, lance integration, streaming, Python ecosystem
- Cons: Initial development time (80-120 hours)
- Recommendation: **PROCEED WITH THIS APPROACH**

**Option D: Fork and Modify Gitstory** ‚ö†Ô∏è
- Pros: Start with working code
- Cons: Rust rewrite needed anyway, loses Python ecosystem benefits
- Recommendation: **ONLY IF RUST EXPERTISE AVAILABLE**

---

## Appendix A: File Reference

**Key Files Analyzed**:
- `/Users/masa/Projects/gitstory/Cargo.toml` (39 lines) - Dependencies
- `/Users/masa/Projects/gitstory/README.md` (405 lines) - Project overview
- `/Users/masa/Projects/gitstory/PRD.md` (510 lines) - Product requirements
- `/Users/masa/Projects/gitstory/src/main.rs` (393 lines) - Entry point
- `/Users/masa/Projects/gitstory/src/extract.rs` (516 lines) - Data extraction
- `/Users/masa/Projects/gitstory/src/invoke.rs` (184 lines) - Claude subprocess
- `/Users/masa/Projects/gitstory/src/cli.rs` (137 lines) - CLI interface
- `/Users/masa/Projects/gitstory/src/output.rs` (401 lines) - Output generation
- `/Users/masa/Projects/gitstory/src/synthesize.rs` (221 lines) - Narrative types
- `/Users/masa/Projects/gitstory/src/error.rs` (129 lines) - Error definitions
- `/Users/masa/Projects/gitstory/src/config.rs` (119 lines) - Configuration
- `/Users/masa/Projects/gitstory/docs/project-narrative-extraction-recipe.md` (414 lines) - Recipe
- `/Users/masa/Projects/gitstory/docs/workport-narrative-opus-synthesis.md` (250 lines) - Example

**Total Lines Analyzed**: 3,318 lines of Rust + 1,333 lines of documentation

---

## Appendix B: Code Snippets

### B.1 Git Extraction Pattern (Reusable)

```rust
// From: extract.rs:185-229
fn extract_commits(repo_path: &Path) -> Result<Vec<CommitInfo>> {
    let output = Command::new("git")
        .args([
            "log",
            "--oneline",
            "--format=%h|%s|%an|%aI",
            "-n",
            "100",
        ])
        .current_dir(repo_path)
        .output()
        .context("Failed to run git log")?;

    let commits = String::from_utf8_lossy(&output.stdout)
        .lines()
        .filter_map(|line| {
            let parts: Vec<&str> = line.splitn(4, '|').collect();
            if parts.len() >= 4 {
                Some(CommitInfo {
                    hash: parts[0].to_string(),
                    message: parts[1].to_string(),
                    author: parts[2].to_string(),
                    date: DateTime::parse_from_rfc3339(parts[3])?.with_timezone(&Utc),
                })
            } else {
                None
            }
        })
        .collect();

    Ok(commits)
}
```

**Python Translation**:
```python
def extract_commits(repo_path: Path, max_commits: int = 100) -> list[CommitInfo]:
    result = subprocess.run(
        ["git", "log", "--oneline", "--format=%h|%s|%an|%aI", "-n", str(max_commits)],
        cwd=repo_path,
        capture_output=True,
        text=True,
        check=True
    )

    commits = []
    for line in result.stdout.strip().split("\n"):
        parts = line.split("|", maxsplit=3)
        if len(parts) >= 4:
            commits.append(CommitInfo(
                hash=parts[0],
                message=parts[1],
                author=parts[2],
                date=datetime.fromisoformat(parts[3])
            ))

    return commits
```

### B.2 Narrative Recipe Prompt Structure

```markdown
# From: docs/project-narrative-extraction-recipe.md

## Phase 1: Data Extraction
- Run: gh issue list --state all --json number,title,state,labels,createdAt
- Run: gh pr list --state all --json number,title,state,mergedAt
- Run: git log --oneline | head -150
- List: docs/, docs/research/

## Phase 2: Pattern Recognition
- Group commits by time period (temporal clustering)
- Identify naming conventions (feat:, fix:, Phase N)
- Trace issue-PR-commit chains via #N references
- Detect inflection points (sudden focus shifts)

## Phase 3: Research Doc Mining
- Find docs >10KB (substantial analysis)
- Extract problem statements, evaluation criteria, decisions
- Match docs to issues/commits by date

## Phase 4: Cross-Artifact Synthesis
- Compare current state to initial PRD
- Identify pivots (sudden architectural changes)
- Trace causality (what triggered pivot, what enabled it)
- Group related artifacts into themes

## Phase 5: Narrative Construction
- Three-act structure: Foundation ‚Üí Challenge ‚Üí Resolution
- Evidence linking: [commit abc], [Issue #42], [docs/file.md]
- Confidence tagging: HIGH/MEDIUM/LOW
- Document unknowns explicitly
```

### B.3 Claude Invocation Pattern (Anti-Pattern for Our Use)

```rust
// From: invoke.rs:57-117
// DO NOT REPLICATE: Subprocess model doesn't work for mcp-vector-search

pub fn spawn_claude(target_dir: &Path, recipe: &str, timeout_secs: u64) -> Result<InvokeResult> {
    let child = Command::new("claude")
        .arg("-p")
        .arg(recipe)
        .current_dir(&target_dir)
        .stdout(Stdio::piped())
        .stderr(Stdio::piped())
        .spawn()?;

    let (tx, rx) = mpsc::channel();
    thread::spawn(move || { tx.send(child.wait_with_output()) });

    match rx.recv_timeout(Duration::from_secs(timeout_secs)) {
        Ok(output) => { /* Process output */ },
        Err(_) => bail!("timed out")
    }
}
```

**Correct Approach for mcp-vector-search**:
```python
# Use Anthropic SDK directly
from anthropic import Anthropic

client = Anthropic(api_key=os.environ["ANTHROPIC_API_KEY"])

# Streaming for progress feedback
with client.messages.stream(
    model="claude-opus-4-6",
    max_tokens=8000,
    messages=[{"role": "user", "content": narrative_prompt}],
    timeout=300.0
) as stream:
    narrative = ""
    for text in stream.text_stream:
        narrative += text
        print(text, end="", flush=True)  # Live progress

    return narrative
```

---

**End of Analysis**

*Generated: 2026-02-20 by Research Agent*
*Research Time: 2.5 hours*
*Files Read: 13 source files, 3,318 lines of code*
*Confidence: HIGH (complete codebase analysis, all key files reviewed)*
